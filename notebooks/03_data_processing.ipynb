{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing \n",
    "\n",
    "Hydrating a system is an essential part of a well-oiled recommendation system. As datasets grow larger and larger, it proved benificial to use specialized computing libraries to help handle the data at scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark \n",
    "\n",
    "Python API access to the Spark computing library that can be used to help process and transform large-scale datasets \n",
    "\n",
    "PySpark provides a convenient SQL API that allows us to write what seems to be SQL queries against large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple PySpark Recommender System "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` serves as an entry point for all Spark functionalitites. \n",
    "\n",
    "You *must* start a session if you want to do any building/manipulating in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/27 13:22:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start a spark session \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"simple-spark-recommender\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") # \n",
    "    .config(\"spark.memory.offHeap.size\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe \n",
    "df = spark.read.csv(\"../data/saved_track_features.csv\", header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|tempo  |type          |id                    |uri                                 |track_href                                              |analysis_url                                                    |duration_ms|time_signature|\n",
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "|0.756       |0.672 |0  |-6.743  |1   |0.0522     |0.464       |3.37e-06        |0.108   |0.739  |114.935|audio_features|2uqYupMHANxnwgeiXTZXzd|spotify:track:2uqYupMHANxnwgeiXTZXzd|https://api.spotify.com/v1/tracks/2uqYupMHANxnwgeiXTZXzd|https://api.spotify.com/v1/audio-analysis/2uqYupMHANxnwgeiXTZXzd|171783     |4             |\n",
      "|0.691       |0.64  |0  |-6.441  |1   |0.369      |0.511       |0.0             |0.409   |0.163  |138.672|audio_features|6MF4tRr5lU8qok8IKaFOBE|spotify:track:6MF4tRr5lU8qok8IKaFOBE|https://api.spotify.com/v1/tracks/6MF4tRr5lU8qok8IKaFOBE|https://api.spotify.com/v1/audio-analysis/6MF4tRr5lU8qok8IKaFOBE|202040     |4             |\n",
      "|0.677       |0.766 |6  |-6.896  |1   |0.0568     |0.0219      |6.81e-06        |0.129   |0.198  |123.062|audio_features|51Of5p3lKZeOg6itfs4og4|spotify:track:51Of5p3lKZeOg6itfs4og4|https://api.spotify.com/v1/tracks/51Of5p3lKZeOg6itfs4og4|https://api.spotify.com/v1/audio-analysis/51Of5p3lKZeOg6itfs4og4|190488     |4             |\n",
      "|0.526       |0.877 |3  |-4.369  |0   |0.033      |0.172       |0.0108          |0.223   |0.436  |145.568|audio_features|1XrSjpNe49IiygZfzb74pk|spotify:track:1XrSjpNe49IiygZfzb74pk|https://api.spotify.com/v1/tracks/1XrSjpNe49IiygZfzb74pk|https://api.spotify.com/v1/audio-analysis/1XrSjpNe49IiygZfzb74pk|255067     |3             |\n",
      "|0.72        |0.88  |9  |-2.834  |1   |0.101      |0.0562      |0.06            |0.153   |0.463  |180.011|audio_features|05WVKTdZhlIMX4qqMLuo0f|spotify:track:05WVKTdZhlIMX4qqMLuo0f|https://api.spotify.com/v1/tracks/05WVKTdZhlIMX4qqMLuo0f|https://api.spotify.com/v1/audio-analysis/05WVKTdZhlIMX4qqMLuo0f|197333     |4             |\n",
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows without truncation (second arg = 0 == no truncation)\n",
    "df.show(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1716 observations in this dataset\n"
     ]
    }
   ],
   "source": [
    "# get n observations\n",
    "print(f\"There are {df.count()} observations in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1716 unique IDs in this datase\n"
     ]
    }
   ],
   "source": [
    "# how many unique ids are there in this dataset\n",
    "n_unique_ids = df.select(\"id\").distinct().count()\n",
    "\n",
    "print(f\"There are {n_unique_ids} unique IDs in this datase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get aggregate tables using `.groupBy()`, `.agg()` and, `.countDistinct()` methods on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "|  1|  224|\n",
      "|  0|  210|\n",
      "|  7|  163|\n",
      "|  5|  156|\n",
      "|  6|  141|\n",
      "|  2|  140|\n",
      "|  9|  132|\n",
      "| 11|  131|\n",
      "|  8|  130|\n",
      "|  4|  126|\n",
      "| 10|  104|\n",
      "|  3|   59|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# what is the most common key -- generate a frequency table\n",
    "key_freq = (\n",
    "    df.groupBy(\"key\")\n",
    "        .agg(countDistinct(\"id\").alias(\"count\"))\n",
    "        .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "key_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
