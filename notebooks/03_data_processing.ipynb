{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrating a System "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrating a system is an essential part of a well-oiled recommendation system. As datasets grow larger and larger, it proved benificial to use specialized computing libraries to help handle the data at scale. \n",
    "\n",
    "**Spark**\n",
    "\n",
    "A natively distributed programming language that is built to scale horizontally and allows for lazy evaluation of expressions\n",
    "\n",
    "Spark employs a driver program that oversees:\n",
    "- Cluster management \n",
    "- Worker nodes each with an executor, cache and tasks\n",
    "\n",
    "**PySpark**\n",
    "\n",
    "Python API access to the Spark computing library that can be used to help process and transform large-scale datasets \n",
    "\n",
    "PySpark provides a convenient SQL API that allows us to write what seems to be SQL queries against large datasets\n",
    "\n",
    "**Lambda Architecture**\n",
    "\n",
    "Daily jos with smaller, and more frequent batch jobs\n",
    "\n",
    "Batch and Speed layers are inversly identified by the frequeny of processing and the volume per run of the data seen\n",
    "- Speed layers may have varying frequencies\n",
    "- One may employ multiple speed layers for varying batch cadences\n",
    "\n",
    "**DataLoaders**\n",
    "\n",
    "Originating from PyTorch but later adopted by other gradient-optimized workflows, DataLoaders allow us to define how data is batched and sent to models for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure for Learning and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Search and ANN Index**\n",
    "\n",
    "Our goal with vectors is to use the latent space and associated similarity metric to quickly retrieve similar items in the space, but how can we make it faster? \n",
    "\n",
    "Inverted Indices \n",
    "- Carefully construct a large hash between tokens of the query and candidates\n",
    "- Good approach for sentences or small-lexicon data\n",
    "- Speed costs are incurred as the dataset grows larger\n",
    "    - Requires two steps \n",
    "    - Similarity distributions may not be correlated with token similarity required \n",
    "\n",
    "ANN Lookups \n",
    "- Allow us to move away from deterministic paradigms and introduce assumptions that help to prune larger datasets\n",
    "\n",
    "\n",
    "**Bloom Filters**\n",
    "**Feature Stores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple PySpark Recommender System "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Familiar with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SparkSession` serves as an entry point for all Spark functionalitites. \n",
    "\n",
    "You *must* start a session if you want to do any building/manipulating in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/09 11:36:02 WARN Utils: Your hostname, srmarshall-mac.local resolves to a loopback address: 127.0.0.1; using 10.64.6.58 instead (on interface en0)\n",
      "24/07/09 11:36:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/09 11:36:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start a spark session \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"simple-spark-recommender\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") # \n",
    "    .config(\"spark.memory.offHeap.size\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe \n",
    "df = spark.read.csv(\"../data/saved_track_features.csv\", header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|tempo  |type          |id                    |uri                                 |track_href                                              |analysis_url                                                    |duration_ms|time_signature|\n",
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "|0.756       |0.672 |0  |-6.743  |1   |0.0522     |0.464       |3.37e-06        |0.108   |0.739  |114.935|audio_features|2uqYupMHANxnwgeiXTZXzd|spotify:track:2uqYupMHANxnwgeiXTZXzd|https://api.spotify.com/v1/tracks/2uqYupMHANxnwgeiXTZXzd|https://api.spotify.com/v1/audio-analysis/2uqYupMHANxnwgeiXTZXzd|171783     |4             |\n",
      "|0.691       |0.64  |0  |-6.441  |1   |0.369      |0.511       |0.0             |0.409   |0.163  |138.672|audio_features|6MF4tRr5lU8qok8IKaFOBE|spotify:track:6MF4tRr5lU8qok8IKaFOBE|https://api.spotify.com/v1/tracks/6MF4tRr5lU8qok8IKaFOBE|https://api.spotify.com/v1/audio-analysis/6MF4tRr5lU8qok8IKaFOBE|202040     |4             |\n",
      "|0.677       |0.766 |6  |-6.896  |1   |0.0568     |0.0219      |6.81e-06        |0.129   |0.198  |123.062|audio_features|51Of5p3lKZeOg6itfs4og4|spotify:track:51Of5p3lKZeOg6itfs4og4|https://api.spotify.com/v1/tracks/51Of5p3lKZeOg6itfs4og4|https://api.spotify.com/v1/audio-analysis/51Of5p3lKZeOg6itfs4og4|190488     |4             |\n",
      "|0.526       |0.877 |3  |-4.369  |0   |0.033      |0.172       |0.0108          |0.223   |0.436  |145.568|audio_features|1XrSjpNe49IiygZfzb74pk|spotify:track:1XrSjpNe49IiygZfzb74pk|https://api.spotify.com/v1/tracks/1XrSjpNe49IiygZfzb74pk|https://api.spotify.com/v1/audio-analysis/1XrSjpNe49IiygZfzb74pk|255067     |3             |\n",
      "|0.72        |0.88  |9  |-2.834  |1   |0.101      |0.0562      |0.06            |0.153   |0.463  |180.011|audio_features|05WVKTdZhlIMX4qqMLuo0f|spotify:track:05WVKTdZhlIMX4qqMLuo0f|https://api.spotify.com/v1/tracks/05WVKTdZhlIMX4qqMLuo0f|https://api.spotify.com/v1/audio-analysis/05WVKTdZhlIMX4qqMLuo0f|197333     |4             |\n",
      "+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+----------------------+------------------------------------+--------------------------------------------------------+----------------------------------------------------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first 5 rows without truncation (second arg = 0 == no truncation)\n",
    "df.show(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1716 observations in this dataset\n"
     ]
    }
   ],
   "source": [
    "# get n observations\n",
    "print(f\"There are {df.count()} observations in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1716 unique IDs in this datase\n"
     ]
    }
   ],
   "source": [
    "# how many unique ids are there in this dataset\n",
    "n_unique_ids = df.select(\"id\").distinct().count()\n",
    "\n",
    "print(f\"There are {n_unique_ids} unique IDs in this datase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get aggregate tables using `.groupBy()`, `.agg()` and, `.countDistinct()` methods on a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=23710Kb max_used=23722Kb free=107361Kb\n",
      " bounds [0x000000010b1d8000, 0x000000010c928000, 0x00000001131d8000]\n",
      " total_blobs=9341 nmethods=8402 adapters=850\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "|  1|  224|\n",
      "|  0|  210|\n",
      "|  7|  163|\n",
      "|  5|  156|\n",
      "|  6|  141|\n",
      "|  2|  140|\n",
      "|  9|  132|\n",
      "| 11|  131|\n",
      "|  8|  130|\n",
      "|  4|  126|\n",
      "| 10|  104|\n",
      "|  3|   59|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# what is the most common key -- generate a frequency table\n",
    "key_freq = (\n",
    "    df.groupBy(\"key\")\n",
    "        .agg(countDistinct(\"id\").alias(\"count\"))\n",
    "        .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "key_freq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Recommender\n",
    "\n",
    "Generate a sample dataset using Spotify listening history to emulate a RFM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/09 13:37:32 WARN Utils: Your hostname, srmarshall-mac.local resolves to a loopback address: 127.0.0.1; using 10.64.6.58 instead (on interface en0)\n",
      "24/07/09 13:37:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/09 13:37:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start a new spark session \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"simple-spark-recommender\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"10g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe \n",
    "df = spark.read.csv(\"../data/listening_history.csv\", header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------+\n",
      "|id                    |timestamp               |\n",
      "+----------------------+------------------------+\n",
      "|5cT3Zcnx3DKezVFAYUPpqg|2024-07-09T14:30:45.920Z|\n",
      "|3l3JdIcEn1lZ6mwnZSO2BV|2024-07-09T14:28:17.056Z|\n",
      "|1CoW9K4Sabt7H8bspY6dI1|2024-07-09T14:24:43.086Z|\n",
      "|5TR5odtJghbnXb9bQv6ubl|2024-07-09T14:21:41.051Z|\n",
      "|2BxwnvgDYhBP3LZbd8tkDu|2024-07-09T14:18:17.057Z|\n",
      "+----------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preview it\n",
    "df.show(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'), ('timestamp', 'timestamp'), ('base_date', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert string time to timestamp \n",
    "df = df.withColumn(\"timestamp\", df[\"timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# confirm\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "\n",
    "# get base date\n",
    "base_date = df.select(F.min('timestamp').alias('min_date')).collect()[0][\"min_date\"]\n",
    "\n",
    "# set base date \n",
    "df = df.withColumn(\"base_date\", lit(base_date))\n",
    "\n",
    "# calcualte recency \n",
    "df_r = df.withColumn(\"recency\", col(\"timestamp\").cast(\"long\") - col(\"base_date\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------+-----------------------+-------+\n",
      "|id                    |timestamp              |base_date              |recency|\n",
      "+----------------------+-----------------------+-----------------------+-------+\n",
      "|5cT3Zcnx3DKezVFAYUPpqg|2024-07-09 10:30:45.92 |2024-07-02 14:44:14.382|589591 |\n",
      "|3l3JdIcEn1lZ6mwnZSO2BV|2024-07-09 10:28:17.056|2024-07-02 14:44:14.382|589443 |\n",
      "|1CoW9K4Sabt7H8bspY6dI1|2024-07-09 10:24:43.086|2024-07-02 14:44:14.382|589229 |\n",
      "|5TR5odtJghbnXb9bQv6ubl|2024-07-09 10:21:41.051|2024-07-02 14:44:14.382|589047 |\n",
      "|2BxwnvgDYhBP3LZbd8tkDu|2024-07-09 10:18:17.057|2024-07-02 14:44:14.382|588843 |\n",
      "+----------------------+-----------------------+-----------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_r.show(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- base_date: timestamp (nullable = false)\n",
      " |-- recency: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_r.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count \n",
    "\n",
    "df_rf = df_r.groupBy(\"id\").agg(count(\"timestamp\").alias(\"frequency\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                  id|frequency|\n",
      "+--------------------+---------+\n",
      "|5TR5odtJghbnXb9bQ...|        1|\n",
      "|2DbwsaYDvznsSkh4c...|        1|\n",
      "|4D32XPyh0L8GX07mU...|        1|\n",
      "|1zZIGnXIcDKU8QIUk...|        1|\n",
      "|7uTqmYA0sSjmDVpQN...|        1|\n",
      "|4KyZLFDRAVvs2naDw...|        1|\n",
      "|4v9Iq4LANThJsxnMN...|        1|\n",
      "|3ecLObZD4KDhSTokl...|        1|\n",
      "|1CoW9K4Sabt7H8bsp...|        1|\n",
      "|3rgTS3933lMWoPiN6...|        1|\n",
      "|26DCohYR7X4PJGRQo...|        1|\n",
      "|1k2pQc5i348DCHwbn...|        1|\n",
      "|1rqqCSm0Qe4I9rUvW...|        1|\n",
      "|0Qwn9VwFUlhrhsNGB...|        1|\n",
      "|3l3JdIcEn1lZ6mwnZ...|        1|\n",
      "|5cT3Zcnx3DKezVFAY...|        1|\n",
      "|4o4y2ZK2pso1lHYAG...|        1|\n",
      "|6u0x5ad9ewHvs3z6u...|        1|\n",
      "|7qTaDOcld0VmBWXnk...|        1|\n",
      "|1SjsVdSXpwm1kTdYE...|        1|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rf.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
